{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import dask\n",
    "from dask import delayed, dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import bokeh\n",
    "import glob\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.config\n",
    "from libpysal.weights import fuzzy_contiguity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./out/singapore/what_cells_are_in_what_cluster_dict.json', 'r') as f:\n",
    "    includes_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = gpd.read_parquet(\"./out/singapore/cluster_dissolved.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = fuzzy_contiguity(chunks, buffering=True, buffer=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = chunks.loc[w.neighbors[2]].plot()\n",
    "chunks.loc[[2]].plot(ax=ax, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(logging={'distributed': 'debug'})\n",
    "\n",
    "daskCluster = LocalCluster(threads_per_worker=2,\n",
    "                n_workers=8, memory_limit='100GB')\n",
    "\n",
    "client = Client(daskCluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates(tessellation):\n",
    "\n",
    "    # Check for duplicates based on UID\n",
    "    duplicates = tessellation[tessellation.duplicated(subset='uID', keep=False)]\n",
    "\n",
    "    # duplicates.to_file( \"./out/\" + place + 'errors.shp', driver='ESRI Shapefile')\n",
    "\n",
    "    # Print the duplicate rows\n",
    "    if len(duplicates) > 0:\n",
    "        raise Exception(\"Tesselation: Duplicate UID entries:\")\n",
    "    elif (tessellation[\"uID\"].isnull().values.any()):\n",
    "        raise Exception(\"uID column include invalid entries\")\n",
    "    else:\n",
    "        print(\"combined_chunk no problems\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queen_out = {}\n",
    "\n",
    "# @delayed\n",
    "# def neigh_look(cell, search_chunks): \n",
    "#     intersects = gpd.sjoin(search_chunks, cell, predicate='touches', how='inner')\n",
    "\n",
    "#     return intersects.uID_left.tolist()\n",
    "\n",
    "\n",
    "# def expand_one_order(n1, main_chunk_ids, periphery_cells_id, order, combined_chunks):\n",
    "    \n",
    "#     red_gdf = combined_chunks[combined_chunks['uID'].isin(periphery_cells_id)]\n",
    "    \n",
    "#     blue_gdf = combined_chunks[combined_chunks['uID'].isin(main_chunk_ids)]\n",
    "    \n",
    "#     # plot the GeoDataFrames on the same plot\n",
    "#     fig, ax = plt.subplots(figsize=(4, 4))\n",
    "#     combined_chunks.plot(ax=ax, color='lightgray')\n",
    "#     blue_gdf.plot(ax=ax, color='blue')\n",
    "#     red_gdf.plot(ax=ax, color='red')\n",
    "#     plt.show()\n",
    "#     print(\"starting expand_one_order, chunk \" + str(n1) + \" order \" + str(order), flush=True)\n",
    "            \n",
    "#     if order == 0:\n",
    "#         return main_chunk_ids\n",
    "\n",
    "#     results = set(main_chunk_ids)\n",
    "    \n",
    "#     search_chunks = combined_chunks[~combined_chunks['uID'].isin(main_chunk_ids + periphery_cells_id)]\n",
    "#     search_chunks_distributed = client.scatter(search_chunks)\n",
    "    \n",
    "#     chunk_size = 320\n",
    "#     periphery_cells_id_chunks = [periphery_cells_id[i:i+chunk_size] for i in range(0, len(periphery_cells_id), chunk_size)]\n",
    "    \n",
    "#     len_sublist = len(periphery_cells_id_chunks)\n",
    "    \n",
    "    \n",
    "#     for i, sublist in enumerate(periphery_cells_id_chunks):\n",
    "\n",
    "#         delay_objs =[]\n",
    "#         new_results = []\n",
    "        \n",
    "#         delay_objs = [neigh_look(combined_chunks[combined_chunks['uID'] == cell_id], search_chunks_distributed) for cell_id in sublist]\n",
    "#         new_results = dask.compute(delay_objs)\n",
    "#         {results.add(new) for new in chain(*new_results[0])}\n",
    "            \n",
    "                    \n",
    "#         print(\"compute done this wave\", flush=True)\n",
    "#         print(\"chunk \" + str(n1) + \" , order \" + str(order) +\" , sublist \" + str(i+1) + \" of \" + str(len_sublist), flush=True)\n",
    "    \n",
    "    \n",
    "#     neighbouring_cells = list(results - set(main_chunk_ids))\n",
    "    \n",
    "#     # find the rows in main_chunk that intersect the buffered polygon\n",
    "    \n",
    "    \n",
    "#     print(\"cells added this wave\", flush=True)\n",
    "    \n",
    "#     return expand_one_order(n1, list(results), neighbouring_cells, order-1, combined_chunks)\n",
    "    \n",
    "# def process_chunk(n1, order, neighbours, chunk):\n",
    "            \n",
    "#     main_chunk = gpd.read_parquet(f\"./out/singapore/tess_cells_in_cluster_{int(n1)}.pq\")\n",
    "#     combined_chunks = main_chunk\n",
    "\n",
    "#     main_chunk_ids = list(main_chunk['uID'])\n",
    "\n",
    "#     for n2 in neighbours:\n",
    "#         neigh_chunk = gpd.read_parquet(f\"./out/singapore/tess_cells_in_cluster_{int(n2)}.pq\")\n",
    "#         combined_chunks = combined_chunks.append(neigh_chunk)\n",
    "\n",
    "#     combined_chunks = combined_chunks.reindex()\n",
    "    \n",
    "#     buffer = gpd.GeoDataFrame(geometry = [chunk.boundary[n1].buffer(5)], crs=main_chunk.crs)\n",
    "    \n",
    "#     # find the rows in main_chunk that intersect the buffered polygon\n",
    "#     intersecting_rows = gpd.sjoin(main_chunk, buffer, how='inner', predicate='intersects')\n",
    "    \n",
    "#     return expand_one_order(n1, main_chunk_ids, intersecting_rows[\"uID\"].tolist(), order, combined_chunks)\n",
    "    \n",
    "\n",
    "# file_pattern = \"./out/singapore/tess_cells_in_cluster_*.pq\"\n",
    "# file_list = glob.glob(file_pattern)\n",
    "# num_files = len(file_list)\n",
    "\n",
    "# queen_out = [process_chunk(n1, 11, w.neighbors[n1], chunks.loc[[n1]]) for n1 in range(num_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_out = {}\n",
    "\n",
    "@delayed\n",
    "def neigh_look(cell, search_chunks): \n",
    "    intersects = gpd.sjoin(search_chunks, cell, predicate='touches', how='inner')\n",
    "\n",
    "    return intersects.uID_left.tolist()\n",
    "\n",
    "\n",
    "def expand_one_order(n1, main_chunk_ids, periphery_cells_id, order, all_chunks):\n",
    "    \n",
    "    # red_gdf = all_chunks[all_chunks['uID'].isin(periphery_cells_id)]\n",
    "    \n",
    "    # blue_gdf = all_chunks[all_chunks['uID'].isin(main_chunk_ids)]\n",
    "    \n",
    "    # # plot the GeoDataFrames on the same plot\n",
    "    # fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    # all_chunks.plot(ax=ax, color='lightgray')\n",
    "    # blue_gdf.plot(ax=ax, color='blue')\n",
    "    # red_gdf.plot(ax=ax, color='red')\n",
    "    # plt.show()\n",
    "    print(\"starting expand_one_order, chunk \" + str(n1) + \" order \" + str(order), flush=True)\n",
    "            \n",
    "    if order == 0:\n",
    "        return main_chunk_ids\n",
    "\n",
    "    results = set(main_chunk_ids)\n",
    "    \n",
    "    search_chunks = all_chunks[~all_chunks['uID'].isin(main_chunk_ids + periphery_cells_id)]\n",
    "    search_chunks_distributed = client.scatter(search_chunks)\n",
    "    \n",
    "    chunk_size = 320\n",
    "    periphery_cells_id_chunks = [periphery_cells_id[i:i+chunk_size] for i in range(0, len(periphery_cells_id), chunk_size)]\n",
    "    \n",
    "    len_sublist = len(periphery_cells_id_chunks)\n",
    "    \n",
    "    \n",
    "    for i, sublist in enumerate(periphery_cells_id_chunks):\n",
    "\n",
    "        delay_objs =[]\n",
    "        new_results = []\n",
    "        \n",
    "        delay_objs = [neigh_look(all_chunks[all_chunks['uID'] == cell_id], search_chunks_distributed) for cell_id in sublist]\n",
    "        new_results = dask.compute(delay_objs)\n",
    "        {results.add(new) for new in chain(*new_results[0])}\n",
    "            \n",
    "                    \n",
    "        print(\"compute done this wave\", flush=True)\n",
    "        print(\"chunk \" + str(n1) + \" , order \" + str(order) +\" , sublist \" + str(i+1) + \" of \" + str(len_sublist), flush=True)\n",
    "    \n",
    "    \n",
    "    neighbouring_cells = list(results - set(main_chunk_ids))\n",
    "    \n",
    "    # find the rows in main_chunk that intersect the buffered polygon\n",
    "    \n",
    "    print(\"cells added this wave\", flush=True)\n",
    "    \n",
    "    return expand_one_order(n1, list(results), neighbouring_cells, order-1, all_chunks)\n",
    "    \n",
    "def process_chunk(n1, order, neighbours, chunk):\n",
    "    \n",
    "    main_chunk = gpd.read_parquet(f\"./out/singapore/tess_cells_in_cluster_{int(n1)}.pq\")\n",
    "\n",
    "    main_chunk_ids = list(main_chunk['uID'])       \n",
    "    \n",
    "    all_chunks = gpd.read_parquet(\"./out/singapore/tessellation.pq\")\n",
    "    \n",
    "    buffer = gpd.GeoDataFrame(geometry = [chunk.boundary[n1].buffer(5)], crs=main_chunk.crs)\n",
    "    \n",
    "    # find the rows in main_chunk that intersect the buffered polygon\n",
    "    intersecting_rows = gpd.sjoin(main_chunk, buffer, how='inner', predicate='intersects')\n",
    "    \n",
    "    return expand_one_order(n1, main_chunk_ids, intersecting_rows[\"uID\"].tolist(), order, all_chunks)\n",
    "    \n",
    "\n",
    "file_pattern = \"./out/singapore/tess_cells_in_cluster_*.pq\"\n",
    "file_list = glob.glob(file_pattern)\n",
    "num_files = len(file_list)\n",
    "\n",
    "queen_out = [process_chunk(n1, 11, w.neighbors[n1], chunks.loc[[n1]]) for n1 in range(num_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = \"./out/singapore/tess_cells_in_cluster_*.pq\"\n",
    "file_list = glob.glob(file_pattern)\n",
    "tess = [gpd.read_parquet(file) for file in file_list]\n",
    "\n",
    "tess = pd.concat(tess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = \"./out/singapore/expanded_cells_in_chunk_*.pq\"\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "for file_path in file_list:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, uID_list in enumerate(queen_out):\n",
    "    out = tess[tess[\"uID\"].isin(uID_list)]\n",
    "    out.plot()\n",
    "    out.to_parquet(f\"./out/singapore/expanded_cells_in_chunk_{i}.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daskCluster.close()\n",
    "client.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
