{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gpd\n",
    "import momepy\n",
    "import networkx as nx\n",
    "import graph_tool.all as gt\n",
    "from scripts.nx2gt import nx2gt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import itertools\n",
    "from shapely.geometry import LineString, point\n",
    "from shapely.wkt import loads\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "place = \"singapore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daskCluster = LocalCluster(threads_per_worker=2,\n",
    "                n_workers=8, memory_limit='100GB')\n",
    "\n",
    "client = Client(daskCluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = gpd.read_parquet(f\"./out/{place}/streets.pq\").explode()\n",
    "\n",
    "streets['edgeID'] = momepy.unique_id(streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the multigraph\n",
    "primal = momepy.gdf_to_nx(streets, length = \"mm_len\", approach='primal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertID_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = itertools.count()\n",
    "\n",
    "for node in primal.nodes():\n",
    "    id = next(counter)\n",
    "    primal.nodes[node]['vertID'] = id\n",
    "    vertID_dict[id] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_vertID_dict = {value: key for key, value in vertID_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primal_scattered = client.scatter(primal)\n",
    "\n",
    "@delayed\n",
    "def compute_ego_graph(n, primal_scattered, radius):\n",
    "    graph = nx2gt(nx.ego_graph(primal_scattered, n, radius, distance=\"mm_len\"))\n",
    "    return (n, graph)\n",
    "\n",
    "tasks = []\n",
    "for n in primal.nodes():\n",
    "    task = compute_ego_graph(n, primal_scattered, 400)\n",
    "    tasks.append(task)\n",
    "    \n",
    "results = dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_graphs = {}\n",
    "\n",
    "for n, ego_graph in results:\n",
    "    ego_graphs[inverted_vertID_dict[n]] = {}\n",
    "    # \n",
    "    ego_graphs[inverted_vertID_dict[n]][\"graph\"] = ego_graph\n",
    "\n",
    "    if len(list(ego_graph.edges())) > 1:\n",
    "        ego_graphs[inverted_vertID_dict[n]][\"weight\"] = ego_graph.edge_properties[\"mm_len\"]\n",
    "    else:\n",
    "        ego_graphs[inverted_vertID_dict[n]][\"weight\"] = None\n",
    "        \n",
    "    ego_graphs[inverted_vertID_dict[n]][\"node\"] = ego_graph.get_vertices()[[i for i,v in enumerate(ego_graph.vp.vertID.get_array()) if v == inverted_vertID_dict[n]][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to a graph-tool graph\n",
    "gtG = nx2gt(primal)\n",
    "\n",
    "gtG.list_properties()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt.GraphView(gtG, vfilt=gt.label_largest_component(gtG))\n",
    "mm_len = gtG.edge_properties[\"mm_len\"]\n",
    "vp, ep = gt.betweenness(g, weight = mm_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeID = g.ep.edgeID.get_array()\n",
    "\n",
    "betweeness_centrality = ep.get_array()\n",
    "\n",
    "geometry = g.ep.geometry.get_array()\n",
    "\n",
    "edges_frame = pd.merge(pd.DataFrame({'edgeID': edgeID, 'betweeness_centrality_edge': betweeness_centrality, 'geometry': geometry}), streets, on='edgeID').drop(columns=[\"geometry_x\"]).rename(columns={\"geometry_y\": \"geometry\"})\n",
    "edges_frame = gpd.GeoDataFrame(edges_frame, geometry=edges_frame['geometry'], crs=streets.crs)\n",
    "edges_frame = edges_frame.sort_values(by=['edgeID'], ascending=True).reset_index(drop=True)\n",
    "edges_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_id = g.vp.vertID.get_array()\n",
    "\n",
    "vp_array = vp.get_array()\n",
    "\n",
    "vertices_frame = pd.DataFrame({'vertID': vertex_id, 'betweeness_centrality_vertex': vp_array})\n",
    "# convert vertID_dict to a DataFrame and rename the index and column\n",
    "vertID_df = pd.DataFrame.from_dict(vertID_dict, orient='index').rename(columns={0: \"y\", 1: \"x\"})\n",
    "# merge vertID_df and vertices on vertID and node, respectively\n",
    "vertices_frame = vertID_df.merge(vertices_frame, left_index=True, right_on='vertID')\n",
    "vertices_frame[\"geometry\"] = vertices_frame.apply(lambda x: point.Point(x[\"x\"], x[\"y\"]), axis=1)\n",
    "vertices_frame = vertices_frame.drop(columns=[\"x\", \"y\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degree = {g.vertex_index[v]: v.in_degree() + v.out_degree() for v in g.vertices()}\n",
    "# convert vertID_dict to a DataFrame and rename the index and column\n",
    "node_degree = pd.DataFrame.from_dict(node_degree, orient='index').rename(columns={0: \"node_degree\"})\n",
    "# merge vertID_df and vertices on vertID and node, respectively\n",
    "vertices_frame = vertices_frame.merge(node_degree, right_index=True, left_on='vertID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness = {k:gt.closeness(ego_graphs[k][\"graph\"], weight=ego_graphs[k][\"weight\"], source=ego_graphs[k][\"node\"]) for k in tqdm(ego_graphs)}\n",
    "closeness_df = pd.DataFrame({'closeness': closeness})\n",
    "closeness_df = closeness_df.applymap(lambda x: x.astype(float)).fillna(0)\n",
    "vertices_frame = vertices_frame.merge(closeness_df, right_index=True, left_on='vertID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_closeness_centrality = gt.closeness(g, weight = mm_len).get_array()\n",
    "global_closeness_centrality = pd.DataFrame({'global_closeness_centrality': global_closeness_centrality})\n",
    "vertices_frame = vertices_frame.merge(global_closeness_centrality, right_index=True, left_on='vertID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_clustering_400m = {k:float(gt.global_clustering(ego_graphs[k][\"graph\"])[0]) for k in tqdm(ego_graphs)}\n",
    "global_clustering_400m  = pd.DataFrame.from_dict(global_clustering_400m , orient='index').rename(columns={0: \"global_clustering_400m\"})\n",
    "vertices_frame = vertices_frame.merge(global_clustering_400m, right_index=True, left_on='vertID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue, eigenvector = gt.eigenvector(g, weight = mm_len, max_iter=100000)\n",
    "eigenvector = pd.DataFrame({'eigenvector': g.ep.edgeID.get_array()})\n",
    "\n",
    "vertices_frame = vertices_frame.merge(eigenvector, right_index=True, left_on='vertID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_centrality_400m = {k:len(ego_graphs[k][\"graph\"].get_vertices())-1 for k in ego_graphs}\n",
    "node_centrality_400m  = pd.DataFrame.from_dict(node_centrality_400m , orient='index').rename(columns={0: \"node_centrality_400m\"})\n",
    "vertices_frame = vertices_frame.merge(node_centrality_400m, right_index=True, left_on='vertID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_frame = gpd.GeoDataFrame(vertices_frame, geometry=vertices_frame['geometry'], crs=streets.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_frame['geometry'] = gpd.points_from_xy(vertices_frame.geometry.y, vertices_frame.geometry.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vertices_frame and edges_frame to GeoDataFrames\n",
    "vertices_frame = gpd.GeoDataFrame(vertices_frame, geometry=vertices_frame['geometry'], crs=streets.crs)\n",
    "edges_frame = gpd.GeoDataFrame(edges_frame, geometry=edges_frame['geometry'], crs=streets.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_frame.to_parquet(f\"./out/{place}/vertices_frame.pq\")\n",
    "edges_frame.to_parquet(f\"./out/{place}/edges_frame.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks = []\n",
    "# for n in primal.nodes():\n",
    "#     task = compute_ego_graph(n, primal_scattered, 2000)\n",
    "#     tasks.append(task)\n",
    "    \n",
    "# results = dask.compute(*tasks)\n",
    "\n",
    "# ego_graphs_2000 = {}\n",
    "\n",
    "# for n, ego_graph in results:\n",
    "#     ego_graphs[inverted_vertID_dict[n]] = {}\n",
    "#     # \n",
    "#     ego_graphs[inverted_vertID_dict[n]][\"graph\"] = ego_graph\n",
    "\n",
    "#     if len(list(ego_graph.edges())) > 1:\n",
    "#         ego_graphs[inverted_vertID_dict[n]][\"weight\"] = ego_graph.edge_properties[\"mm_len\"]\n",
    "#     else:\n",
    "#         ego_graphs[inverted_vertID_dict[n]][\"weight\"] = None\n",
    "        \n",
    "#     ego_graphs[inverted_vertID_dict[n]][\"node\"] = ego_graph.get_vertices()[[i for i,v in enumerate(ego_graph.vp.vertID.get_array()) if v == inverted_vertID_dict[n]][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # pickle the object\n",
    "# with open('ego_graphs_2000.pickle', 'wb') as f:\n",
    "#     pickle.dump(ego_graphs_2000, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straightness_df = {k:0 for k in tqdm(ego_graphs_2000)}\n",
    "\n",
    "# def euclidean_dist(x1, y1, x2, y2):\n",
    "#     return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "# def bravo(target, k, vertID_dict, network_dist):\n",
    "#     euclidean_distance = euclidean_dist(vertID_dict[k][0], vertID_dict[k][1], vertID_dict[target][0], vertID_dict[target][1])\n",
    "#     return euclidean_distance / network_dist\n",
    "\n",
    "# for k in tqdm(ego_graphs_2000):\n",
    "#     ego_graph = ego_graphs_2000[k][\"graph\"]\n",
    "#     straightness = 0\n",
    "#     sp = gt.shortest_distance(ego_graph, k, target=gt.shortest_distance weights=ego_graph.edge_properties[\"mm_len\"])\n",
    "\n",
    "#     if len(sp.get_array()) > 0 and len(G) > 1:\n",
    "#         for target, value in enumerate(sp):\n",
    "#             if k != target:\n",
    "#                 network_dist = sp_scattered[target]\n",
    "#                 straightness += bravo(target, k, vertID_dict, network_dist)\n",
    "#         straightness_df[k] = straightness * (1.0 / (len(vertID_dict.keys()) - 1.0))\n",
    "#     else:\n",
    "#         straightness_df[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too slow\n",
    "\n",
    "# def euclidean_dist(x1, y1, x2, y2):\n",
    "#     return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "# def bravo(target, n, vertID_dict_scattered, network_dist):\n",
    "#     euclidean_distance = euclidean_dist(vertID_dict_scattered[n][0], vertID_dict_scattered[n][1], vertID_dict_scattered[target][0], vertID_dict_scattered[target][1])\n",
    "#     return euclidean_distance / network_dist\n",
    "\n",
    "# @dask.delayed\n",
    "# def alpha(G, n, vertID_dict_scattered):\n",
    "#     straightness = 0\n",
    "#     sp = gt.shortest_distance(G, n, weights=G.edge_properties[\"mm_len\"])\n",
    "#     sp_scattered = sp\n",
    "\n",
    "#     if len(sp.get_array()) > 0 and len(G) > 1:\n",
    "#         for target, value in enumerate(sp):\n",
    "#             if n != target:\n",
    "#                 network_dist = sp_scattered[target]\n",
    "#                 straightness += bravo(target, n, vertID_dict_scattered, network_dist)\n",
    "#         straightness_df = straightness * (1.0 / (len(vertID_dict_scattered.keys()) - 1.0))\n",
    "#     else:\n",
    "#         straightness_df = 0\n",
    "    \n",
    "#     return n, straightness_df\n",
    "\n",
    "# def split_list(lst, chunk_size):\n",
    "#     return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\\\n",
    "        \n",
    "# def straightness_centrality(G, vertID_dict):\n",
    "    \n",
    "#     # chunked_list = split_list(list(G.iter_vertices()), 32)\n",
    "#     # G_scattered = client.scatter(G)\n",
    "#     # vertID_dict_scattered = client.scatter(vertID_dict)\n",
    "#     # result=[]\n",
    "    \n",
    "#     # for chunk in chunked_list:   \n",
    "#     #     delayed_objs = [alpha(G_scattered, n, vertID_dict_scattered) for n in chunk]\n",
    "#     #     new_results = client.compute(delayed_objs)\n",
    "#         # result.append(new_results)\n",
    "    \n",
    "    \n",
    "#     chunked_list = split_list(list(G.iter_vertices()), 32)\n",
    "#     G_scattered = client.scatter(G)\n",
    "#     vertID_dict_scattered = client.scatter(vertID_dict)\n",
    "#     results=[]\n",
    "\n",
    "#     for chunk in chunked_list:   \n",
    "#         delayed_objs = [alpha(G_scattered, n, vertID_dict_scattered) for n in chunk]\n",
    "#         new_results = client.compute(delayed_objs)\n",
    "#         gathered_results = client.gather(new_results)\n",
    "#         results.append(gathered_results)\n",
    "        \n",
    "#     return results\n",
    "\n",
    "# straightness_df = straightness_centrality(gtG, vertID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the multigraph\n",
    "dual = momepy.gdf_to_nx(streets, approach='dual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertID_dict = {}\n",
    "\n",
    "# counter = itertools.count()\n",
    "\n",
    "# for node in dual.nodes():\n",
    "#     id = next(counter)\n",
    "#     dual.nodes[node]['vertID'] = id\n",
    "#     vertID_dict[id] = node\n",
    "    \n",
    "# inverted_vertID_dict = {value: key for key, value in vertID_dict.items()}\n",
    "\n",
    "# dual_scattered = client.scatter(dual)\n",
    "\n",
    "# @delayed\n",
    "# def compute_ego_graph(n, dual_scattered, radius):\n",
    "#     graph = nx2gt(nx.ego_graph(dual_scattered, n, radius, distance=\"angle\"))\n",
    "#     return (n, graph)\n",
    "\n",
    "# # tasks = []\n",
    "# # for n in dual.nodes():\n",
    "# #     task = compute_ego_graph(n, dual_scattered, 400)\n",
    "# #     tasks.append(task)\n",
    "    \n",
    "# # results = dask.compute(*tasks)\n",
    "\n",
    "# # define a function to update vertID_dict every 5000 entries\n",
    "\n",
    "# chunk_size = 1000\n",
    "\n",
    "# chunks = [list(dual.nodes())[i:i+chunk_size] for i in range(0, len(dual.nodes()), chunk_size)]\n",
    "\n",
    "# len_sublist = len(chunks)\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for i, sublist in enumerate(chunks):\n",
    "\n",
    "#     delay_objs =[]\n",
    "#     new_results = []\n",
    "    \n",
    "#     delay_objs = [compute_ego_graph(n, dual_scattered, 400) for n in sublist]\n",
    "#     new_results = dask.compute(delay_objs)\n",
    "#     {results.append(tup) for tup in new_results}\n",
    "\n",
    "# ego_graphs = {}\n",
    "\n",
    "# for n, ego_graph in results:\n",
    "#     ego_graphs[inverted_vertID_dict[n]] = {}\n",
    "#     # \n",
    "#     ego_graphs[inverted_vertID_dict[n]][\"graph\"] = ego_graph\n",
    "\n",
    "#     if len(list(ego_graph.edges())) > 1:\n",
    "#         ego_graphs[inverted_vertID_dict[n]][\"weight\"] = ego_graph.edge_properties[\"angle\"]\n",
    "#     else:\n",
    "#         ego_graphs[inverted_vertID_dict[n]][\"weight\"] = None\n",
    "        \n",
    "#     ego_graphs[inverted_vertID_dict[n]][\"node\"] = ego_graph.get_vertices()[[i for i,v in enumerate(ego_graph.vp.vertID.get_array()) if v == inverted_vertID_dict[n]][0]]\n",
    "    \n",
    "# # Convert it to a graph-tool graph\n",
    "# gtG = nx2gt(dual)\n",
    "\n",
    "# gtG.list_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # pickle gtG as dual_graph\n",
    "# def pickle_graph(graph, filename):\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         pickle.dump(graph, f)\n",
    "        \n",
    "# pickle_graph(gtG, f\"./out/{place}/dual_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness = {k:gt.closeness(ego_graphs[k][\"graph\"], weight=ego_graphs[k][\"weight\"], source=ego_graphs[k][\"node\"]) for k in tqdm(ego_graphs)}\n",
    "# closeness_df = pd.DataFrame({'closeness': closeness})\n",
    "# closeness_df = closeness_df.applymap(lambda x: x.astype(float)).fillna(0)\n",
    "\n",
    "# angle = gtG.edge_properties[\"angle\"]\n",
    "\n",
    "# global_closeness_centrality = gt.closeness(g, weight = angle).get_array()\n",
    "\n",
    "# vp, ep = gt.betweenness(g, weight = angle)\n",
    "\n",
    "# dual_vertices_frame = pd.DataFrame({'vertID': vertex_id, 'angcloseness400': closeness_df, 'closeness_global_ang': global_closeness_centrality})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dual_vertices_frame.to_parquet(f\"./out/{place}/dual_vertices_frame.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daskCluster.close()\n",
    "client.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
